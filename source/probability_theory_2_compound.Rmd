---
resampling_with:
    ed2_fname: 10-Chap-6
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.6
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{r setup, include=FALSE}
source("_common.R")
```

# Probability Theory, Part 2: Compound Probability {#sec-compound-probability}

:::{.callout-warning}
## Draft page partially ported from original PDF

This page is an automated and partial import from the [original second-edition
PDF](https://resample.com/content/text/10-Chap-6.pdf).

We are in the process of updating this page for formatting, and porting any
code from the original [RESAMPLING-STATS
language](http://www.statistics101.net) to Python and R.

Feel free to read this version for the sense, but expect there to be multiple
issues with formatting.

We will remove this warning when the page has adequate formatting, and we have
ported the code.
:::

## Introduction

In this chapter we will deal with what are usually called "probability
problems" rather than the "statistical inference problems" discussed in
later chapters. The difference is that for probability problems we begin
with a knowledge of the properties of the universe with which we are
working. (See @sec-what-is-resampling on the definition of resampling.)

## Examples of basic problems in probability

### A Poker Problem: One Pair (Two of a Kind) {#sec-one-pair}

What is the chance that the first five cards chosen from a deck of 52
(bridge/poker) cards will contain two (and only two) cards of the same
denomination (two 3's for example)? (Please forgive the rather sterile
unrealistic problems in this and the other chapters on probability. They
reflect the literature in the field for 300 years. We'll get more
realistic in the statistics chapters.)

We shall estimate the odds the way that gamblers have estimated gambling
odds for thousands of years. First, check that the deck is a
standard deck and is not missing any cards. (Overlooking such small but crucial
matters often leads to errors in science.) Shuffle thoroughly until you are
satisfied that the cards are randomly distributed. (It is surprisingly hard to
shuffle well.) Then deal five cards, and mark down whether the hand does or
does not contain a pair of the same denomination.

At this point, we must decide whether three of a kind, four of a kind or two
pairs meet our criterion for a pair. Since our criterion is "two and only two,"
we decide *not* to count them.

Then replace the five cards in the deck, shuffle, and deal again. Again
mark down whether the hand contains one pair of the same denomination.
Do this many times. Then count the number of hands with one pair, and
figure the proportion (as a percentage) of all hands.

```{python echo=FALSE}
n_deals = 25
```

@tbl-one-pair has the results of `r py$n_deals` hands of this procedure.

```{python echo=FALSE}
from itertools import product
suit_chars = ['\N{Black Spade Suit}',
    '\N{White Heart Suit}',
    '\N{White Diamond Suit}',
    '\N{Black Club Suit}']
denom = ['Ace'] + [str(s) for s in range(2, 11)] + ['Jack', 'Queen', 'King']
values = list(range(1, 14)) * 4
deck = [f'{d} {s}' for s, d in product(suit_chars, denom)]
deck_values = dict(zip(deck, values))
```

```{python echo=FALSE, results="asis", message=FALSE}
import numpy as np
import pandas as pd
seed = 1937
rnd = np.random.default_rng(seed)
df = pd.DataFrame(columns=['Card 1',
                           'Card 2',
                           'Card 3',
                           'Card 4',
                           'Card 5',
                           'One pair?'],
                  index=pd.Index(range(1, n_deals + 1), name='Hand'))
for i in range(1, n_deals + 1):
    cards = rnd.choice(deck, size=5, replace=False)
    df.loc[i, 'Card 1':'Card 5'] = cards
    values = [deck_values[c] for c in cards]
    counts = np.bincount(values)
    df.loc[i, 'One pair?'] = 'Yes' if np.sum(counts == 2) == 1 else 'No'

def add_pct(df):
    pct = int(np.round(np.sum(df['One pair?'] == 'Yes') / len(df) * 100))
    n_cols = len(df.columns)
    df.loc['', :] = [''] * n_cols
    df.loc['**% Yes**', :] = [''] * (n_cols - 1) + [f'{pct}%']
    return pct

pct = add_pct(df)
cards_tab = df.to_markdown(tablefmt="grid")
print(f'{cards_tab}\n\n: Results of {n_deals} hands '
      'for the problem "one pair" {#tbl-one-pair}')
```

In this series of `r py$n_deals` experiments, `r py$pct` percent of the hands
contained one pair, and therefore `r py$pct / 100` is our estimate (for the
time being) of the probability that one pair will turn up in a poker hand. But
we must notice that this estimate is based on only `r py$n_deals` hands, and
therefore might well be fairly far off the mark (as we shall soon see).

This experimental "resampling" estimation does not require a deck of cards. For
example, one might create a 52-sided die, one side for each card in the deck,
and roll it five times to get a "hand." But note one important part of the
procedure: No single "card" is allowed to come up twice in the same set of five
spins, just as no single card can turn up twice or more in the same hand. If
the same "card" did turn up twice or more in a dice experiment, one could
pretend that the roll had never taken place; this procedure is necessary to
make the dice experiment analogous to the actual card-dealing situation under
investigation. Otherwise, the results will be slightly in error. This type of
sampling is "sampling without replacement," because each card is *not replaced*
in the deck prior to dealing the next card (that is, prior to the end of the
hand).

We could also approach this problem using random numbers from the computer to
simulate the values.

Let us first make some numbers from which to sample.  We want to simulate a
deck of playing cards analogous to the real cards we used previously. We don't
need to simulate all the features of a deck, but only the features that matter
for the problem at hand.  In our case, the feature that matters is the face
value.  We require a deck with four "1"s, four "2"s, etc., up to four "13"s,
where 1 is an Ace, and 13 is a King. The suits don't matter for our present
purposes.

We first first make {{< var an_array >}} to represent the face values in one
suit.

```{python opts.label="py_ed"}
# Card values 1 through 13 (1 up to, not including 14).
one_suit = np.arange(1, 14)
one_suit
```

```{r opts.label="r_ed"}
one_suit <- 1:13
one_suit
```

We have the face values for one suit, but we need the face values for whole
deck of cards — four suits.  We do this by making a new {{< var array >}} that
consists of four repeats of `one_suit`:

```{python opts.label="py_ed"}
# Repeat the one_suit array four times
deck = np.repeat(one_suit, 4)
deck
```

```{r opts.label="r_ed"}
# Repeat the one_suit vector four times
deck = rep(one_suit, 4)
deck
```

At this point we have a complete deck in the variable `deck` . But that "deck"
is
[in the same order as a new deck of cards]{.r}
[ordered by value, first ones (Aces) then 2s and so on]{.python}.
If we do not shuffle the deck, the results will be predictable.  Therefore, we
would like to select five of these "cards" (52 values) at random.  There are
two ways of doing this.  The first is to use the
[`sample`]{.r}'rnd.choice`]{.python} tool in the familiar way, to choose 5
values at random from this strictly ordered deck.  We want to draw these cards
*without replacement* (of which more later).  *Without replacement* means that
once we have drawn a particular value, we cannot draw that value a second time
— just as you cannot get the same card twice in a hand when the dealer deals
you a hand of five cards.

::: python
So far, each of our uses of `rnd.choice` has done sampling *with replacement*,
where you *can* get the same item more than once in a particular sample.  Here we need *without replacement*.  `rnd.choice` has an argument you can send, called `replace`, to tell it whether to replace values when drawing the sample.  We have not used that argument so far, because the default is `True` — sampling *with replacement*.  Here we need to use the argument — `replace=False` — to get sampling *without replacement*.
:::

```{python opts.label="py_ed"}
# One hand, sampling from the deck without replacement.
hand = rnd.choice(deck, size=5, replace=False)
hand
```

::: r
We are using `sample`, and the *default* behavior of `sample` is to sample
*without replacement*.  Up until now we have always told R to change that
default behavior, using the `replace=TRUE` argument to `sample`.  This tells
`sample` to sample *with replacement*.  Now we want to sample *with
replacement*, so we leave out the `replace=TRUE` to let sample do its default
sampling, *without replacement*.  That is, when we do not specify `replace=`, R
assumes `replace=FALSE` — sampling without replacement.
:::

```{r opts.label="r_ed"}
# One hand, sampling from the deck without replacement.
hand <- sample(deck, size=5)
hand
```

The above is one way to get a random hand of five cards from the deck.  Another
way is to use [`sample`]{.r}[the `rnd.permuted` function]{.python} to shuffle
the whole `deck` of 52 "cards" into a random order, just as a dealer would
shuffle the deck before dealing.  Then we could take — for example — the first
five cards from the shuffled deck.

::: python
`rnd.permuted` is another function (actually, a method) of `rnd`, that takes an array as input, and produces a version of the array, where the elements are in random order.  It is exactly equivalent to fair and complete shuffling of a deck of cards, where the cards should be in a completely random order.
:::

```{python opts.label="py_ed"}
# Shuffle the whole 52 card deck.
shuffled = rnd.permuted(deck)
# The "cards" are now in random order.
shuffled
```

::: sample
We can use `sample` to shuffle the deck.  Remember, by default, `sample` takes
its sample *without replacement*.   So, if we take a random sample of 52 cards,
without replacement, this is just the same as putting the cards into a random
order.  We could tell `sample` that we want a sample of 52 values, using
`size=52`. But in fact, `sample` assumes, by default, if we do not specify
`size`, that you want a sample that is the same size as the vector you pass in.
That means that, for our case, where we are passing in a vector `deck` of 52
values, it assumes we mean `size=52`, and sends us back the result we want, a complete sample without replacement, and therefore, a shuffled vector.
:::

```{r opts.label="r_ed"}
# Shuffle the whole 52 card deck.
shuffled <- sample(deck)
# The "cards" are now in random order.
shuffled
```

Now we can get our `hand` by taking the first five cards from the `deck`:

```{python opts.label="py_ed"}
# Select the first five "cards" from the shuffled deck.
hand = shuffled[:5]
hand
```

```{r opts.label="r_ed"}
# Select the first five "cards" from the shuffled deck.
hand <- shuffled[1:5]
hand
```

The two procedures are completely equivalent, of taking a random sample of five
cards from the `deck` (as we did further above), or of shuffling the entire
`deck` and then taking the first five "cards".  Either is a valid way of
getting five cards at random from the `deck`.  It's up to us which to choose —
we slightly prefer to shuffle and take the first five, because it is more like
the physical procedure of shuffling the deck and dealing, but which you prefer,
is up to you.

Choosing the shuffle deal way, the {{< var cell >}} to generate one hand is:

```{python opts.label="py_ed"}
shuffled = rnd.permuted(deck)
hand = shuffled[:5]
hand
```

```{r opts.label="r_ed"}
shuffled <- sample(deck)
hand <- shuffled[1:5]
hand
```

Without doing anything further, we could run this {{< var cell >}} many times,
and each time, we could note down whether the particular `hand` had exactly one pair or not.

@tbl-one-pair-numbers has the result of running that procedure `r py$n_deals`
times:

```{python echo=FALSE, results="asis", message=FALSE}
df = pd.DataFrame(columns=['Card 1',
                           'Card 2',
                           'Card 3',
                           'Card 4',
                           'Card 5',
                           'One pair?'],
                  index=pd.Index(range(1, n_deals + 1), name='Hand'))
deck = np.repeat(np.arange(1, 14), 4)
for i in range(1, n_deals + 1):
    cards = rnd.choice(deck, size=5, replace=False)
    df.loc[i, 'Card 1':'Card 5'] = cards
    counts = np.bincount(cards)
    df.loc[i, 'One pair?'] = 'Yes' if np.sum(counts == 2) == 1 else 'No'

pct = add_pct(df)
cards_tab = df.to_markdown(tablefmt="grid")
print(f'{cards_tab}\n\n: Results of {n_deals} hands '
      'using random numbers {#tbl-one-pair-numbers}')
```

<!---
Now we must find out if there is one (and only one) pair; we do this with the
MULTIPLES statement — the "2" in that statement indicates that it is a
duplicate, rather than a singleton or triplicate or quadruplicate that we are
testing for — and we put the result in location D. Next we SCORE in location z
how many pairs there are, the number in each trial being either zero, one, or
two. (The reason we cannot put the result of the MULTIPLES operation directly
into the scorecard vector z is that only the SCORE command accumulates results
from trial to trial rather than over-writing the result of the past trial with
the current one.) And with that we end a single trial.
-->

Now let's use {{< var lang >}} to do the full job of dealing many hands and
finding pairs in each one.  To do this, we repeat the procedure above using a
the `for` loop. This commands the program to do ten thousand repeats of the
statements in the "loop" [between the start `{` and end `}` curly
braces]{.r}[(indented statements)]{.python}.

When those 10000 repetitions are over, the computer moves on to count (`sum`)
the number of "1's" in the score-keeping {{< var array >}} `z`, each "1"
indicating a hand with a pair.  We store this count at location `k`. We divide
`k` by 10000 to get the *proportion* of hands that had one pair, and we
[`message`]{.r}[`print`]{.python}
the result of `k` to the screen.

<!---
Introduce bincount / tabulate / repeat
-->

::: {.notebook name="one_pair" title="One pair"}

::: nb-only
This is a simulation to find the probability of exactly one pair in a poker
hand of five cards.
:::

```{python opts.label="py_ed"}
import numpy as np
rnd = np.random.default_rng()
```

```{python opts.label="py_ed"}
# Create a bucket (vector) called a with four "1's," four "2's," four "3's,"
# etc., to represent a deck of cards
one_suit = np.arange(1, 14)
one_suit
```

```{python opts.label="py_ed"}
# Repeat values for one suit four times to make a 52 card deck of values.
deck = np.repeat(one_suit, 4)
deck
```

```{python opts.label="py_ed"}
# Array to store result of each trial.
z = np.zeros(10000)

# Repeat the following steps 10000 times
for i in range(10000):
    # Shuffle the deck
    shuffled = rnd.permuted(deck)

    # Take the first five cards to make a hand.
    hand = shuffled[:5]

    # How many pairs?
    # Counts for each card rank.
    counts = np.bincount(hand)
    n_pairs = np.sum(counts == 2)

    # Keep score of # of pairs
    z[i] = n_pairs

    # End loop, go back and repeat

# How often was there 1 pair?
k = np.sum(z == 1)

# Convert to proportion.
kk = k / 10000

# Show the result.
print(kk)
```

```{r opts.label="r_ed"}
# Create a bucket (vector) called a with four "1's," four "2's," four "3's,"
# etc., to represent a deck of cards
one_suit = 1:13
one_suit
```

```{r opts.label="r_ed"}
# Repeat values for one suit four times to make a 52 card deck of values.
deck = rep(one_suit, 4)
deck
```

```{r opts.label="r_ed"}
# Vector to store result of each trial.
z <- numeric(10000)

# Repeat the following steps 10000 times
for (i in 1:10000) {
    # Shuffle the deck
    shuffled <- sample(deck)

    # Take the first five cards to make a hand.
    hand = shuffled[1:5]

    # How many pairs?
    # Counts for each card rank.
    counts <- tabulate(hand)
    n_pairs <- sum(counts == 2)

    # Keep score of # of pairs
    z[i] <- n_pairs

    # End loop, go back and repeat
}

# How often was there 1 pair?
k <- sum(z == 1)

# Convert to proportion.
kk = k / 10000

# Show the result.
message(kk)
```

:::

```{r echo=FALSE}
rkk <- round(get_var('kk'), 3)
```

In one run of the program, the result in `kk` was `r rkk`, so our estimate
would be that the probability of a single pair is `r rkk`.

How accurate are these resampling estimates? The accuracy depends on the
*number of hands* we deal — the more hands, the greater the accuracy. If
we were to examine millions of hands, 42 percent would contain a pair
each; that is, the chance of getting a pair in the long run is 42
percent. It turns out the estimate of `r py$pct` percent based on 
`r py$n_deals` hands in @tbl-one-pair is fairly close to the long-run
estimate, though whether or not it is close *enough* depends on one's needs of
course. If you need great accuracy, deal many more hands.

A note on the `a`s, `b`s, `c`s in the above program, etc.: These "variables"
are called "{{< var array >}}"s in {{< var lang >}}. A *{{< var array >}}* is
an array (sequence) of elements that gets filled with numbers as {{< var lang
>}} conducts its operations. When {{< var lang >}} completes a single trial
these {{< var array >}}s are generally reset except for the "score" {{< var
array >}} (here labeled `z`) which keeps track of the result of each trial.

To help keep things straight (though the program does not require it), we
usually use `z` to name the {{< var array >}} that collects all the trial
results, and `k` to denote our overall summary results. Or you could call it
something like `scoreboard` — it's up to you.

How many trials (hands) should be made for the estimate? There is no
easy answer.[^how-many-trials] One useful device is to run several (perhaps
ten) equal sized sets of trials, and then examine whether the proportion of
pairs found in the entire group of trials is very different from the
proportions found in the various subgroup sets. If the proportions of pairs in
the various subgroups differ greatly from one another or from the overall
proportion, then keep running additional larger subgroups of trials until the
variation from one subgroup to another is sufficiently small for your purposes.
While such a procedure would be impractical using a deck of cards or any other
physical means, it requires little effort with the computer and {{< var lang
>}}.

[^how-many-trials]: One simple rule-of-thumb is to quadruple the original
    number. The reason for quadrupling is that four times as many iterations
    (trials) of this resampling procedure give *twice* as much accuracy (as
    measured by the standard deviation, the most frequent measurement of
    accuracy). That is, the error decreases with the square root of the number
    of iterations. If you see that you need *much* more accuracy, then
    *immediately* increase the number of iterations even more than four times
    — perhaps ten or a hundred times.

### Another Introductory Poker Problem

Which is more likely, a poker hand with two pairs, or a hand with three
of a kind? This is a *comparison* problem, rather than a problem in
*absolute* estimation as was the previous example.

In a series of 100 "hands" that were "dealt" using random numbers, four
hands contained two pairs, and two hands contained three of a kind. Is
it safe to say, on the basis of these 100 hands, that hands with two
pairs are more frequent than hands with three of a kind? To check, we
deal another 300 hands. Among them we see fifteen hands with two pairs
(3.75 percent) and eight hands with three of a kind (2 percent), for a
total of nineteen to ten. Although the difference is not enormous, it is
reasonably clear-cut. Another 400 hands might be advisable, but we shall
not bother.

Earlier I obtained forty-four hands with *one* pair each out of 100
hands, which makes it quite plain that *one* pair is more frequent than
*either* two pairs or three-of-a-kind. Obviously, we need *more* hands
to compare the odds in favor of two pairs with the odds in favor of
three-of-a-kind than to compare those for one pair with those for either
two pairs or three-of-a-kind. Why? Because the difference in odds
between one pair, and either two pairs or three-of-a-kind, is much
greater than the difference in odds between two pairs and
three-of-a-kind. This observation leads to a general rule: The closer
the odds between two events, the *more trials* are needed to determine
which has the higher odds.

Again it is interesting to compare the odds with the formulaic
mathematical computations, which are 1 in 21 (4.75 percent) for a hand
containing two pairs and 1 in 47 (2.1 percent) for a hand containing
three-of-a-kind — not too far from the estimates of .0375 and .02
derived from simulation.

To handle the problem with the aid of the computer, we simply need to
estimate the proportion of hands having triplicates and the proportion
of hands with two pairs, and compare those estimates.

To estimate the hands with three-of-a-kind, we can use a program just
like "One Pair" earlier, except instructing the MUL- TIPLES statement to
search for triplicates instead of duplicates. The program, then, is:

```{python opts.label="py_ed"}
triples_per_trial = np.zeros(10000)

# Repeat the following steps 10000 times
for i in range(10000):
    # Shuffle the deck
    shuffled = rnd.permuted(deck)

    # Take the first five cards.
    hand = shuffled[:5]

    # How many triples?
    counts = np.bincount(hand)
    n_triples = np.sum(counts == 3)

    # Keep score of # of triples
    triples_per_trial[i] = n_triples

    # End loop, go back and repeat

# How often was there 1 pair?
n_triples = np.sum(triples_per_trial == 1)

# Convert to proportion
print(n_triples / 10000)
```

Note: The file "3kind" on the Resampling Stats software disk contains
this set of commands.

To estimate the probability of getting a two-pair hand, we revert to the
original program (counting pairs), except that we examine all the
results in the score-keeping vector `z` for hands in which we had *two* pairs,
instead of *one* .

```{python opts.label="py_ed"}
pairs_per_trial = np.zeros(10000)

# Repeat the following steps 10000 times
for i in range(10000):
    # Shuffle the deck
    shuffled = rnd.permuted(deck)

    # Take the first five cards.
    hand = shuffled[:5]

    # How many pairs?
    # Counts for each card rank.
    counts = np.bincount(hand)
    n_pairs = np.sum(counts == 2)

    # Keep score of # of pairs
    pairs_per_trial[i] = n_pairs

    # End loop, go back and repeat

# How often were there 2 pairs?
n_two_pairs = np.sum(pairs_per_trial == 2)

# Convert to proportion
print(n_two_pairs / 10000)
```

Note: The file "2pair" on the Resampling Stats software disk contains
this set of commands.

For efficiency (though efficiency really is not important here because
the computer performs its operations so cheaply) we could develop both
estimates in a single program by simply generating 10000 hands, and count
the number with three-of-a-kind and the number with two pairs.

Before we leave the poker problems, we note a difficulty with Monte
Carlo simulation. The probability of a royal flush is so low (about one
in half a million) that it would take much computer time to compute. On
the other hand, considerable inaccuracy is of little matter. Should one
care whether the probability of a royal flush is 1/100,000 or 1/500,000?

## The concepts of replacement and non-replacement

In the poker example above, we *did not replace* the first card we drew.
If we were to replace the card, it would leave the probability the same
before the second pick as before the first pick. That is, the
conditional probability remains the same. *If we replace, conditions do
not change.* But if we do not replace the item drawn, the probability
changes from one moment to the next. (Perhaps refresh your mind with the
examples in the discussion of conditional probability including
@sec-example-four-events)

If we sample with replacement, the sample drawings remain *independent* of each
other — a topic addressed in @sec-independence.

In many cases, a key decision in modeling the situation in which we are
interested is whether to sample with or without replacement. The choice
must depend on the characteristics of the situation.

There is a close connection between the lack of finiteness of the
concept of universe in a given situation, and sampling with replacement.
That is, when the universe (population) we have in mind is not small, or
has no conceptual bounds at all, then the probability of each successive
observation remains the same, and this is modeled by sampling with
replacement. ("Not finite" is a less expansive term than "infinite,"
though one might regard them as synonymous.)

@sec-infinite-universes discusses problems whose appropriate concept of a universe is
finite, whereas @sec-finite-universes discusses problems whose appropriate concept
of a universe is not finite. This general procedure will be discussed
several times, with examples included.
